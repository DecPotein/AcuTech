{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f116f4",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a5865",
   "metadata": {},
   "source": [
    "## Loading of Dataset & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "audio_train_files = os.listdir('C:/Users/Jerome/Desktop/Jerome/Programmierung Projekte/Thesis-Projekt/ACS-master-final/backend/data/dataset_train_unclean/')\n",
    "audio_test_files = os.listdir('C:/Users/Jerome/Desktop/Jerome/Programmierung Projekte/Thesis-Projekt/ACS-master-final/backend/data/dataset_test_svm/')\n",
    "\n",
    "train = pd.read_csv('C:/Users/Jerome/Desktop/Jerome/Programmierung Projekte/Thesis-Projekt/ACS-master-final/backend/data/metadata_train_unclean.csv')\n",
    "submission = pd.read_csv('C:/Users/Jerome/Desktop/Jerome/Programmierung Projekte/Thesis-Projekt/ACS-master-final/backend/data/metadata_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1a94c",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6df6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "\n",
    "def clean_filename(fname, string):   \n",
    "    file_name = fname.split('/')[1]\n",
    "    if file_name[:2] == '__':        \n",
    "        file_name = string + file_name\n",
    "    return file_name\n",
    "\n",
    "# Generate mfcc features with mean and standard deviation\n",
    "def get_mfcc(name, path):\n",
    "    data, _ = librosa.core.load(path + name, sr = SAMPLE_RATE)\n",
    "    assert _ == SAMPLE_RATE\n",
    "    try:\n",
    "        ft1 = librosa.feature.mfcc(data, sr = SAMPLE_RATE, n_mfcc = 30)\n",
    "        ft2 = librosa.feature.zero_crossing_rate(data)[0]\n",
    "        ft3 = librosa.feature.spectral_rolloff(data)[0]\n",
    "        ft4 = librosa.feature.spectral_centroid(data)[0]\n",
    "        ft5 = librosa.feature.spectral_contrast(data)[0]\n",
    "        ft6 = librosa.feature.spectral_bandwidth(data)[0]\n",
    "        ft1_trunc = np.hstack((np.mean(ft1, axis = 1), np.std(ft1, axis = 1), skew(ft1, axis = 1), np.max(ft1, axis = 1), np.median(ft1, axis = 1), np.min(ft1, axis = 1)))\n",
    "        ft2_trunc = np.hstack((np.mean(ft2), np.std(ft2), skew(ft2), np.max(ft2), np.median(ft2), np.min(ft2)))\n",
    "        ft3_trunc = np.hstack((np.mean(ft3), np.std(ft3), skew(ft3), np.max(ft3), np.median(ft3), np.min(ft3)))\n",
    "        ft4_trunc = np.hstack((np.mean(ft4), np.std(ft4), skew(ft4), np.max(ft4), np.median(ft4), np.min(ft4)))\n",
    "        ft5_trunc = np.hstack((np.mean(ft5), np.std(ft5), skew(ft5), np.max(ft5), np.median(ft5), np.min(ft5)))\n",
    "        ft6_trunc = np.hstack((np.mean(ft6), np.std(ft6), skew(ft6), np.max(ft6), np.median(ft6), np.max(ft6)))\n",
    "        return pd.Series(np.hstack((ft1_trunc, ft2_trunc, ft3_trunc, ft4_trunc, ft5_trunc, ft6_trunc)))\n",
    "    except:\n",
    "        print('Bad file')\n",
    "        return pd.Series([0] * 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e673c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_labels(preds, i2c, k = 3):\n",
    "    ans = []\n",
    "    ids = []\n",
    "    for p in preds:\n",
    "        idx = np.argsort(p)[::-1]\n",
    "        ids.append([i for i in idx[:k]])\n",
    "        ans.append(' '.join([i2c[i] for i in idx[:k]]))\n",
    "\n",
    "    return ans, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97264f",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "train_data['fileName'] = train['fileName']\n",
    "test_data = pd.DataFrame()\n",
    "test_data['fileName'] = audio_test_files\n",
    "\n",
    "train_data = train_data['fileName'].progress_apply(get_mfcc, path = 'C:/Users/Jerome/Desktop/Jerome/Programmierung Projekte/Thesis-Projekt/ACS-master-final/backend/data/dataset_train_unclean/')\n",
    "print('Finished loading MFCC for training dataset')\n",
    "test_data = test_data['fileName'].progress_apply(get_mfcc, path = 'C:/Users/Jerome/Desktop/Jerome/Programmierung Projekte/Thesis-Projekt/ACS-master-final/backend/data/dataset_test_svm/')\n",
    "print('Finished loading MFCC for testing dataset')\n",
    "\n",
    "train_data['fileName'] = train['fileName']\n",
    "test_data['fileName'] = audio_test_files\n",
    "\n",
    "train_data['className'] = train['className']\n",
    "test_data['className'] = np.zeros((len(audio_test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(['className', 'fileName'], axis = 1)\n",
    "feature_names = list(X.columns)\n",
    "X = X.values\n",
    "labels = np.sort(np.unique(train_data.className.values))\n",
    "num_class = len(labels)\n",
    "c2i = {}\n",
    "i2c = {}\n",
    "for i, c in enumerate(labels):\n",
    "    c2i[c] = i\n",
    "    i2c[i] = c\n",
    "y = np.array([c2i[x] for x in train_data.className.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.drop(['className', 'fileName'], axis = 1)\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc04e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling for PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c01f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for dimension reduction\n",
    "pca = PCA(n_components = 65).fit(X_scaled)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bccd9f6",
   "metadata": {},
   "source": [
    "## SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bc7434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an SVM model\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "\n",
    "clf = SVC(kernel = 'rbf', probability = True)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print('Primary Training Accuracy: ', accuracy_score(clf.predict(X_train), y_train) * 100, '%')\n",
    "print('Primary Testing Accuracy: ', accuracy_score(clf.predict(X_val), y_val) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = [0.001, 0.01, 0.1, 1, 10]\n",
    "gamma_grid = [0.001, 0.01, 0.1, 1, 10]\n",
    "param_grid = {'C': C_grid, 'gamma' : gamma_grid}\n",
    "\n",
    "grid = GridSearchCV(SVC(kernel = 'rbf'), param_grid, cv = 3, scoring = \"accuracy\")\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c96682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal model\n",
    "clf = SVC(kernel = 'rbf', C = 4, gamma = 0.01, probability = True)\n",
    "\n",
    "history = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Actual Training Accuracy: ', accuracy_score(clf.predict(X_train), y_train) * 100, \"%\")\n",
    "print('Actual Testing Accuracy: ', accuracy_score(clf.predict(X_val), y_val) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_classification(random_state = 65)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 65)\n",
    "clf = SVC(random_state = 65)\n",
    "clf.fit(X_test, y_test)\n",
    "SVC(random_state = 65)\n",
    "plot_confusion_matrix(clf, X_train, y_train)  \n",
    "plt.title(\"SVM Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "438d67235aeee2b8696b3da762607795780e379b31b02f6127c7d3d84ea8b479"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
